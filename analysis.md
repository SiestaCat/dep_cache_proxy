# Updated Analysis of **DepCacheProxy** (`dep_cache_proxy`)

This document corrects and expands the design of **DepCacheProxy** to reflect that in `cache/objects` **complete** `node_modules` or `vendor` folders are **not** stored, but only **individual objects** (files) identified by their hash. Each bundle’s index maps relative paths to file hashes, and the final ZIP is generated by referencing those blobs.

---

## Table of Contents

- [Updated Analysis of **DepCacheProxy** (`dep_cache_proxy`)](#updated-analysis-of-depcacheproxy-dep_cache_proxy)
  - [Table of Contents](#table-of-contents)
  - [1. Objectives and Context](#1-objectives-and-context)
  - [2. Functional and Non-Functional Requirements](#2-functional-and-non-functional-requirements)
    - [2.1 Functional Requirements (FR)](#21-functional-requirements-fr)
    - [2.2 Non-Functional Requirements (NFR)](#22-non-functional-requirements-nfr)
  - [3. High-Level Architecture (DDD + SOLID)](#3-high-level-architecture-ddd--solid)
  - [4. Main Components](#4-main-components)
    - [4.1 Client (`dep_cache_proxy_client`)](#41-client-dep_cache_proxy_client)
      - [4.1.1 Goals](#411-goals)
      - [4.1.2 Modules](#412-modules)
      - [4.1.3 CLI Arguments](#413-cli-arguments)
    - [4.2 Server (`dep_cache_proxy_server`)](#42-server-dep_cache_proxy_server)
      - [4.2.1 Goals](#421-goals)
      - [4.2.2 Modules](#422-modules)
      - [4.2.3 Server CLI Arguments](#423-server-cli-arguments)
  - [5. Domain Model and Hashing](#5-domain-model-and-hashing)
    - [5.1 Hash Constants (`hash_constants.py`)](#51-hash-constants-hash_constantspy)
    - [5.2 Entity: `DependencySet` (`dependency_set.py`)](#52-entity-dependencyset-dependency_setpy)
    - [5.3 Interface: `ICacheRepository` (`cache_repository.py`)](#53-interface-icacherepository-cache_repositorypy)
    - [5.4 File Blob Logic (`blob_storage.py`)](#54-file-blob-logic-blob_storagepy)
  - [6. Directory Structure in Cache](#6-directory-structure-in-cache)
  - [7. Full Workflow](#7-full-workflow)
  - [8. Implementation Details and Pseudocode](#8-implementation-details-and-pseudocode)
    - [8.1 Client Pseudocode](#81-client-pseudocode)
    - [8.2 Server Pseudocode](#82-server-pseudocode)
    - [8.3 Common Helper Functions](#83-common-helper-functions)
      - [8.3.1 `ZipUtil` for ZIP from Blobs (`zip_util.py`)](#831-ziputil-for-zip-from-blobs-zip_utilpy)
  - [9. HTTP API and Route Schema](#9-http-api-and-route-schema)
    - [9.1 Routes](#91-routes)
    - [9.2 `CacheRequestDTO`](#92-cacherequestdto)
    - [9.3 `CacheResponseDTO`](#93-cacheresponsedto)
  - [10. Test Structure](#10-test-structure)
    - [10.1 Unit Tests](#101-unit-tests)
    - [10.2 Integration Tests](#102-integration-tests)
    - [10.3 Functional Tests](#103-functional-tests)
    - [10.4 End-to-End Tests](#104-end-to-end-tests)
  - [11. Use Cases and Test Scenarios](#11-use-cases-and-test-scenarios)
    - [11.1 Scenario 1: NPM Cache Hit](#111-scenario-1-npm-cache-hit)
    - [11.2 Scenario 2: Composer Cache Miss](#112-scenario-2-composer-cache-miss)
    - [11.3 Scenario 3: Unsupported Version without Docker](#113-scenario-3-unsupported-version-without-docker)
    - [11.4 Scenario 4: Unsupported Version with Docker](#114-scenario-4-unsupported-version-with-docker)
  - [12. Security, Scalability, and Common Pitfalls](#12-security-scalability-and-common-pitfalls)
    - [12.1 Security](#121-security)
    - [12.2 Scalability](#122-scalability)
    - [12.3 Common Pitfalls](#123-common-pitfalls)
  - [13. Ease of Adding New Managers](#13-ease-of-adding-new-managers)
  - [14. Conclusions](#14-conclusions)

---

## 1. Objectives and Context

This analysis documents the design of **DepCacheProxy** with the following priorities:

1. **Cache dependencies via file blobs**

   * Each file (for example, `file.js`, `subfolder/file.js`) inside `node_modules/` or `vendor/` is stored as an individual “object” in `cache/objects`, using its content hash as the file name.
   * Avoid storing the entire folder structure in `cache/objects`.

2. **Path → Hash Index**

   * For each set of dependencies (bundle), generate a JSON index that maps relative paths (e.g., `"file.js"`, `"subfolder/file.js"`) to file hashes.
   * Store that index in `cache/indexes`, named `<bundle_hash>.<manager>.<manager_version>.index`.

3. **ZIP Reconstructed from Blobs**

   * When the server needs to deliver the ZIP to the client, it reads the index and for each entry retrieves the corresponding blob from `cache/objects/<h0h1>/<h2h3>/<file_hash>` and adds it to the ZIP under the appropriate path.
   * The generated ZIP is stored in `cache/bundles/<bundle_hash>.zip` (optional to persist if you are willing to regenerate on demand).

4. **Version and Docker Support**

   * Same as before: validate requested versions against those supported. If there's a mismatch and `--use-docker-on-version-mismatch=true`, use Docker to install inside a container.

5. **DDD + SOLID Structure**

   * Separate layers: domain, application, and infrastructure.
   * Modules follow single responsibility.

6. **Testing**

   * Define unit tests, integration tests, functional tests, and end-to-end tests.

7. **Extensibility to New Managers**

   * Use a Factory/Strategy pattern for `DependencyInstaller`.

8. **Implementation in Python**

   * Use constants for hashing algorithm (`HASH_ALGORITHM`, `HASH_BLOCK_SIZE`).
   * Manage temporary directories for installing dependencies.

---

## 2. Functional and Non-Functional Requirements

### 2.1 Functional Requirements (FR)

1. **FR1**: The client CLI (`dep_cache_proxy_client`) must accept:

   * `<endpoint_url>` (server URL).
   * `<manager>` (e.g., `npm`, `composer`).
   * `--apikey=<APIKEY>` (optional if the server is public).
   * `--files=<file1>,<file2>` (definition file + lockfile).
   * **Version options** (optional on client, required on server):

     * `--node-version=<VERSION>` and `--npm-version=<VERSION>` (for `npm`).
     * `--php-version=<VERSION>` (for `composer`).

2. **FR2**: The server CLI (`dep_cache_proxy_server`) must accept:

   * `<port>` (HTTP port).
   * `--cache_dir=<CACHE_DIR>` (base directory for cache).
   * `--supported-versions-node=<NODE_VER1>:<NPM_VER1>,<NODE_VER2>:<NPM_VER2>,...` (optional; if not specified, any version is supported).
   * `--supported-versions-php=<PHP_VER1>,<PHP_VER2>,...` (optional; if not specified, any version is supported).
   * `--use-docker-on-version-mismatch` (boolean; use Docker if version unsupported).
   * `--is_public` (boolean; default `false`).
   * `--api-keys=<KEY1>,<KEY2>,...` (required if `--is_public=false`).

3. **FR3**: Calculation of **bundle hash**:

   * Use constants:

     ```python
     HASH_ALGORITHM = "sha256"
     HASH_BLOCK_SIZE = 8192
     ```
   * Include in the hash (deterministically):

     1. `manager` string (e.g., `"npm"`).
     2. Requested versions (e.g., `"node=14.20.0"`, `"npm=6.14.13"`, `"php=8.1.0"`).
     3. Contents of each file (`.json` and `.lock`), sorted alphabetically by file name.

4. **FR4**: File blob caching:

   * In `cache/objects/`, store only **individual files**, not folders.
   * Each file gets its own hash (SHA256) based on content.
   * The on-disk path of a file blob `file.js` with `file_hash = "aabb12323232322..."` is:

     ```
     cache/objects/aa/bb/aabb12323232322...
     ```

     (two levels based on the first two pairs of hex characters of `file_hash`).

5. **FR5**: Index for each bundle:

   * For each bundle (bundle\_hash generated from `package.json`, `package.lock`, versions, etc.), create a JSON file in:

     ```
     cache/indexes/<bundle_hash>.<manager>.<manager_version>.index
     ```
   * Index format (JSON):

     ```json
     {
       "file.js": "aabb12323232322...",
       "subfolder/file.js": "eecc1232132132121...",
       "..."
     }
     ```
   * This index maps relative path → file blob hash.

6. **FR6**: Reconstructed ZIP:

   * To deliver the ZIP to the client:

     1. Read the index of the bundle.
     2. For each `"relative_path"` and `"file_hash"` pair:

        * Read blob from `cache/objects/<h0h1>/<h2h3>/<file_hash>`.
        * Add it to the ZIP with `arcname=relative_path`.
     3. Generate `<bundle_hash>.zip` and store it in:

        ```
        cache/bundles/<bundle_hash>.zip
        ```

     * Optionally, you can regenerate it on every request or keep it persisted.

7. **FR7**: Version validation on server:

   * For `npm`: if `--supported-versions-node` is specified, the tuple `(node_version, npm_version)` must exist in `supported_versions_node`.
   * For `composer`: if `--supported-versions-php` is specified, `php_version` must exist in `supported_versions_php`.
   * If no supported versions are specified for a package manager, any version is accepted and installation will be performed using the current manager version of the host.
   * If versions are specified and there's a mismatch:

     * If `--use-docker-on-version-mismatch=false`: respond `400 Bad Request`.
     * If `--use-docker-on-version-mismatch=true`: use Docker to install dependencies in a container with the requested version.

8. **FR8**: Client behavior:

   * Calculate **bundle hash** locally (using the constants).
   * Send JSON to the server with:

     * `manager`, `hash`, `files` (Base64), `versions`.
   * Receive `download_url` and `cache_hit`.
   * Download the ZIP and extract it into the local dependency folder (`node_modules/` or `vendor/`).

9. **FR9**: Testing:

   * **Unit Tests**:

     * `HashCalculator`.
     * `ApiKeyValidator`.
     * `InstallerFactory`.
     * `ZipUtil` (listing, checksums, ZIP creation).
   * **Integration Tests**:

     * `HandleCacheRequest` (hit and miss).
     * Verify `cache/objects`, `cache/indexes`, and indexes are created.
   * **Functional Tests**:

     * Run `dep_cache_proxy_client` → FastAPI server.
     * Test `GET /download/{bundle_hash}.zip`.
   * **End-to-End Tests**:

     * Using Docker for both server and client.
     * Verify complete extraction.

### 2.2 Non-Functional Requirements (NFR)

1. **NFR1 (Efficiency)**:

   * Streamed hashing (`HASH_BLOCK_SIZE`).
   * Avoid duplicate blobs (if the same file already exists, do not overwrite).

2. **NFR2 (Scalability)**:

   * Thread-safe concurrency: avoid two simultaneous requests for the same bundle producing duplicate blobs.
   * Locks per `bundle_hash` during storage.

3. **NFR3 (Extensibility)**:

   * Factory pattern for `DependencyInstaller` (to support adding Yarn, Pip, etc.).
   * Document steps to add a new manager.

4. **NFR4 (Security)**:

   * Sanitize `manager` and `versions`.
   * Use `--ignore-scripts` and `--no-scripts` for NPM/Composer.
   * Parameterize Docker commands to prevent injection.

5. **NFR5 (Portability)**:

   * Python 3.x on Linux/macOS.
   * Docker optional when installation with unsupported versions is required locally.

6. **NFR6 (Maintainability)**:

   * Follow PEP8, use type hints, include docstrings.
   * Tests implemented in `pytest`.
   * DDD + SOLID separation.

7. **NFR7 (Availability)**:

   * Server does not handle TLS natively; typically run behind a TLS proxy.

---

## 3. High-Level Architecture (DDD + SOLID)

```
dep_cache_proxy/
├── client/
│   ├── cli.py
│   ├── hash_calculator.py
│   ├── http_client.py
│   └── downloader.py
├── server/
│   ├── domain/
│   │   ├── hash_constants.py
│   │   ├── dependency_set.py
│   │   ├── cache_repository.py
│   │   ├── blob_storage.py
│   │   ├── installer.py
│   │   └── zip_util.py
│   ├── infrastructure/
│   │   ├── file_system_cache_repository.py
│   │   ├── api_key_validator.py
│   │   └── docker_utils.py
│   ├── application/
│   │   ├── dtos.py
│   │   └── handle_cache_request.py
│   └── interfaces/
│       └── main.py
├── cache/
│   ├── objects/
│   │   └── (blobs by hash)
│   ├── indexes/
│   │   └── (bundle → {relative_path: file_hash} indexes)
│   └── bundles/
│       └── (ZIPs for each bundle)
├── tests/
│   ├── unit/
│   ├── integration/
│   ├── functional/
│   └── e2e/
└── README.md
```

* `hash_constants.py`: hash algorithm constants (`sha256`) and block size.
* `blob_storage.py`: logic to store and retrieve file blobs in `cache/objects`.
* `zip_util.py`: reconstruct a ZIP by reading blobs according to the index.
* `handle_cache_request.py`: orchestrates hit/miss, storing blobs, writing index, and generating ZIP.

---

## 4. Main Components

### 4.1 Client (`dep_cache_proxy_client`)

#### 4.1.1 Goals

* Read local files (`.json`, `.lock`).
* Calculate **bundle hash** locally.
* Send JSON payload to the server.
* Receive `download_url` and `cache_hit`.
* Download the ZIP and extract it into the local folder.

#### 4.1.2 Modules

* **`client/cli.py`**: argument parsing and main flow.
* **`client/hash_calculator.py`**: hashing using `HASH_ALGORITHM` and `HASH_BLOCK_SIZE`.
* **`client/http_client.py`**: sends request to server and handles response.
* **`client/downloader.py`**: downloads and extracts the ZIP.

#### 4.1.3 CLI Arguments

```bash
dep_cache_proxy_client <endpoint_url> <manager> \
  --apikey=<APIKEY> \
  --files=<file1>,<file2> \
  [--node-version=<VERSION>] [--npm-version=<VERSION>] \
  [--php-version=<VERSION>] \
  [--timeout=<seconds>]
```

* `<endpoint_url>`: base URL of the server (e.g., `http://host:port/api`).
* `<manager>`: `npm`, `composer`, etc.
* `--apikey`: API key if applicable.
* `--files`: list of files (definition + lockfile).
* `--node-version` and `--npm-version` (only for `npm`).
* `--php-version` (only for `composer`).
* `--timeout` (default 60s).

---

### 4.2 Server (`dep_cache_proxy_server`)

#### 4.2.1 Goals

* Receive cache requests.
* Validate API key (if not public).
* Verify requested versions.
* On a “cache miss”:

  1. Create a temporary directory.
  2. Install dependencies (locally or in Docker).
  3. For each resulting file, compute its `file_hash` (SHA256) and store the blob in `cache/objects`.
  4. Create a JSON index in `cache/indexes` mapping relative path → `file_hash`.
  5. Reconstruct the ZIP from blobs and save it in `cache/bundles/<bundle_hash>.zip`.
  6. Respond with `download_url`.
* On a “cache hit”:

  * Return immediately with `download_url` without regenerating.

#### 4.2.2 Modules

* **`server/domain/hash_constants.py`**: `HASH_ALGORITHM`, `HASH_BLOCK_SIZE`.
* **`server/domain/dependency_set.py`**: calculates **bundle hash**.
* **`server/domain/cache_repository.py`**: interface `ICacheRepository`.
* **`server/domain/blob_storage.py`**: logic to write and read blobs in `cache/objects`.
* **`server/domain/installer.py`**: `DependencyInstaller`, `NpmInstaller`, `ComposerInstaller`.
* **`server/domain/zip_util.py`**: create ZIP from blobs.
* **`server/infrastructure/file_system_cache_repository.py`**: implements `ICacheRepository` on the filesystem.
* **`server/infrastructure/api_key_validator.py`**: validates API keys.
* **`server/infrastructure/docker_utils.py`**: helper functions for Docker.
* **`server/application/dtos.py`**: `CacheRequestDTO`, `CacheResponseDTO`.
* **`server/application/handle_cache_request.py`**: orchestrates hit/miss, blob storage, index writing, and ZIP creation.
* **`server/interfaces/main.py`**: FastAPI entry point; argument parsing.

#### 4.2.3 Server CLI Arguments

```bash
dep_cache_proxy_server <port> \
  --cache_dir=<CACHE_DIR> \
  [--supported-versions-node=<NODE_VER1>:<NPM_VER1>,<NODE_VER2>:<NPM_VER2>,...] \
  [--supported-versions-php=<PHP_VER1>,<PHP_VER2>,...] \
  [--use-docker-on-version-mismatch] \
  [--is_public] \
  [--api-keys=<KEY1>,<KEY2>,...]
```

* `<port>`: integer (e.g., `8080`).
* `--cache_dir`: base cache directory.
* `--supported-versions-node`: e.g., `14.20.0:6.14.13,16.15.0:8.5.0` (optional; if not specified, any Node/NPM version is supported).
* `--supported-versions-php`: e.g., `8.1.0,7.4.0` (optional; if not specified, any PHP version is supported).
* `--use-docker-on-version-mismatch`: use Docker to install dependencies if version unsupported.
* `--is_public`: run as a public server (no API key).
* `--api-keys`: comma-separated list of valid keys (required if `--is_public=false`).

---

## 5. Domain Model and Hashing

### 5.1 Hash Constants (`hash_constants.py`)

```python
# server/domain/hash_constants.py

HASH_ALGORITHM = "sha256"
HASH_BLOCK_SIZE = 8192
```

### 5.2 Entity: `DependencySet` (`dependency_set.py`)

```python
# server/domain/dependency_set.py
import hashlib
from typing import Dict
from .hash_constants import HASH_ALGORITHM, HASH_BLOCK_SIZE

class DependencySet:
    """
    Represents the unique combination of:
      - manager: "npm" or "composer"
      - file_contents: {"package.json": b"...", "package.lock": b"..."}
      - versions: {"node":"14.20.0","npm":"6.14.13"} or {"php":"8.1.0"}
    Calculates a SHA256 hash including manager, versions, and file contents.
    """
    def __init__(self, manager: str, file_contents: Dict[str, bytes], versions: Dict[str, str]):
        self.manager = manager
        self.file_contents = file_contents
        self.versions = versions
        self.hash = self.calculate_hash()

    def calculate_hash(self) -> str:
        sha = hashlib.new(HASH_ALGORITHM)
        sha.update(self.manager.encode("utf-8"))
        sha.update(b"\n")
        for name in sorted(self.file_contents.keys()):
            content = self.file_contents[name]
            idx = 0
            while idx < len(content):
                chunk = content[idx: idx + HASH_BLOCK_SIZE]
                sha.update(chunk)
                idx += HASH_BLOCK_SIZE
        for key in sorted(self.versions.keys()):
            val = self.versions[key]
            sha.update(f"{key}={val}\n".encode("utf-8"))
        return sha.hexdigest()
```

### 5.3 Interface: `ICacheRepository` (`cache_repository.py`)

```python
# server/domain/cache_repository.py
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional

class CacheObject:
    """
    Represents metadata of a bundle:
      - bundle_hash: hash of the bundle
      - manager: "npm" or "composer"
      - manager_version: e.g., "14.20.0_6.14.13" or "8.1.0"
    """
    def __init__(self, bundle_hash: str, manager: str, manager_version: str):
        self.bundle_hash = bundle_hash
        self.manager = manager
        self.manager_version = manager_version

class ICacheRepository(ABC):
    @abstractmethod
    def exists_bundle(self, bundle_hash: str) -> bool:
        """
        Checks if a ZIP already exists for this bundle hash.
        """
        pass

    @abstractmethod
    def get_index(self, bundle_hash: str, manager: str, manager_version: str) -> Optional[Path]:
        """
        Returns the path to the JSON index if it exists, or None.
        """
        pass

    @abstractmethod
    def save_index(self, bundle_hash: str, manager: str, manager_version: str, index_data: dict) -> None:
        """
        Saves the JSON index in cache/indexes.
        """
        pass

    @abstractmethod
    def save_blob(self, file_hash: str, content: bytes) -> None:
        """
        Saves the file blob in cache/objects/<h0h1>/<h2h3>/<file_hash>, if not already present.
        """
        pass

    @abstractmethod
    def get_blob_path(self, file_hash: str) -> Path:
        """
        Returns the absolute path to the blob given its hash.
        """
        pass

    @abstractmethod
    def save_bundle_zip(self, bundle_hash: str, zip_content_path: Path) -> None:
        """
        Saves (or overwrites) the generated ZIP in cache/bundles/<bundle_hash>.zip.
        """
        pass

    @abstractmethod
    def get_bundle_zip_path(self, bundle_hash: str) -> Path:
        """
        Returns the path to the bundle’s ZIP if it exists.
        """
        pass
```

### 5.4 File Blob Logic (`blob_storage.py`)

```python
# server/domain/blob_storage.py
import os
import hashlib
from pathlib import Path
from typing import Tuple

from .hash_constants import HASH_ALGORITHM, HASH_BLOCK_SIZE

class BlobStorage:
    """
    Encapsulates logic for storing and retrieving file blobs in cache/objects.
    """

    def __init__(self, objects_dir: Path):
        self.objects_dir = objects_dir
        self.objects_dir.mkdir(parents=True, exist_ok=True)

    def compute_file_hash(self, file_path: Path) -> str:
        """
        Calculates the SHA256 of a file’s content in blocks.
        """
        sha = hashlib.new(HASH_ALGORITHM)
        with open(file_path, "rb") as f:
            while True:
                chunk = f.read(HASH_BLOCK_SIZE)
                if not chunk:
                    break
                sha.update(chunk)
        return sha.hexdigest()

    def get_blob_path(self, file_hash: str) -> Path:
        """
        Physical path of the blob: <objects_dir>/<h0h1>/<h2h3>/<file_hash>
        """
        h0_2 = file_hash[0:2]
        h2_4 = file_hash[2:4]
        dir_path = self.objects_dir / h0_2 / h2_4
        dir_path.mkdir(parents=True, exist_ok=True)
        return dir_path / file_hash

    def save_blob(self, file_path: Path) -> str:
        """
        Reads the file at file_path, calculates its hash, and saves its content
        in cache/objects/.../<file_hash> if not already present. Returns file_hash.
        """
        file_hash = self.compute_file_hash(file_path)
        dest = self.get_blob_path(file_hash)
        if not dest.is_file():
            # Only write if it does not already exist
            with open(file_path, "rb") as src, open(dest, "wb") as dst:
                while True:
                    chunk = src.read(HASH_BLOCK_SIZE)
                    if not chunk:
                        break
                    dst.write(chunk)
        return file_hash

    def read_blob(self, file_hash: str) -> bytes:
        """
        Returns the content of the blob with file_hash.
        """
        path = self.get_blob_path(file_hash)
        with open(path, "rb") as f:
            return f.read()
```

---

## 6. Directory Structure in Cache

Under `<cache_dir>`, there are three main subfolders:

```
<cache_dir>/
├── objects/
│   ├── aa/bb/aabb12323232322...   # file blob with hash aabb...
│   ├── ee/cc/eecc1232132132121... # file blob with hash eecc...
│   └── ...
├── indexes/
│   ├── <bundle_hash>.npm.14.20.0_6.14.13.index
│   ├── <bundle_hash2>.composer.8.1.0.index
│   └── ...
└── bundles/
    ├── <bundle_hash>.zip
    ├── <bundle_hash2>.zip
    └── ...
```

* **`objects/`**

  * Stores individual file blobs (no file extensions), named by `file_hash`.
  * Two directory levels: first two characters `h[0:2]`, next two `h[2:4]`.

* **`indexes/`**

  * Each index is a JSON file with content:

    ```json
    {
      "file.js": "aabb12323232322...",
      "subfolder/file.js": "eecc1232132132121...",
      ...
    }
    ```
  * Named: `<bundle_hash>.<manager>.<manager_version>.index`.
  * Example: `ab12cd34.npm.14.20.0_6.14.13.index`.

* **`bundles/`**

  * Contains the final ZIP files:

    ```
    <bundle_hash>.zip
    ```
  * Generated from the index and blobs.

---

## 7. Full Workflow

1. **Client** invocation:

   ```bash
   dep_cache_proxy_client https://server:8080/api npm \
     --apikey=MY_API_KEY \
     --files=package.json,package.lock \
     --node-version=14.20.0 \
     --npm-version=6.14.13
   ```

   * Reads `package.json` and `package.lock`.
   * Builds `versions = {"node":"14.20.0","npm":"6.14.13"}`.
   * Calculates **bundle\_hash** using `DependencySet.calculate_hash()`.
   * Constructs multipart form data request:
     * `manager`: "npm"
     * `hash`: calculated bundle hash
     * `versions`: JSON string `{"node":"14.20.0","npm":"6.14.13"}`
     * `file[]`: array of file uploads (package.json and optionally package-lock.json)
   * Sends `POST https://server:8080/api/v1/cache` with:
     * Headers: `Authorization: Bearer MY_API_KEY`, `Content-Type: multipart/form-data`
     * The multipart form data above.

2. **Server: `POST /v1/cache`**

   * FastAPI receives the multipart form data request in `cache_endpoint()`.
   * **Validate API Key** (if `--is_public=false`).
   * Parse multipart form data:
     * Extract form fields: `manager`, `hash`, `versions` (parse JSON string)
     * Extract file uploads from `file[]` array and identify manifest/lockfile by filename
   * **Validate Manager** (`npm` or `composer`).
   * **Validate Versions**:

     * For `npm`: if `supported_versions_node` is provided, `(node_version, npm_version)` must be in `supported_versions_node`.
     * For `composer`: if `supported_versions_php` is provided, `php_version` must be in `supported_versions_php`.
     * If no supported versions are specified for a package manager, any version is accepted.
     * If versions are specified and there's a mismatch:

       * If `use_docker_on_version_mismatch=false`: → `400 Bad Request: "Unsupported version"`.
       * If `use_docker_on_version_mismatch=true`: → set `use_docker = True`.
   * `bundle_hash = dto.hash`; `manager_version = "14.20.0_6.14.13"` (in this example).
   * Check if a ZIP already exists in `cache/bundles/<bundle_hash>.zip`:

     * If it exists → **Cache Hit**:

       * Respond `{ "download_url": "http://.../download/<bundle_hash>.zip", "cache_hit": true }`.
     * If it does not exist → **Cache Miss**:

       1. Create `temp_dir = mkdtemp(prefix=bundle_hash)`.
       2. Decode Base64 files and write them in `temp_dir`.
       3. Create folder `temp_dir/node_modules` (or `temp_dir/vendor`) after installation:

          * If `use_docker`:

            ```bash
            docker run --rm \
              -v <temp_dir>:/usr/src/app \
              -w /usr/src/app \
              node:<node_version> \
              sh -c "npm ci --ignore-scripts --no-audit --cache .npm_cache"
            ```

            or for Composer:

            ```bash
            docker run --rm \
              -v <temp_dir>:/app \
              -w /app \
              composer:<php_version> \
              sh -c "composer install --no-dev --prefer-dist --no-scripts"
            ```
          * If **without Docker**:

            * NPM:

              ```bash
              npm ci --ignore-scripts --no-audit --cache .npm_cache
              ```
            * Composer:

              ```bash
              composer install --no-dev --prefer-dist --no-scripts
              ```
       4. **Blobify Files**:

          * Instantiate `blob_storage = BlobStorage(cache_dir/"objects")`.
          * Initialize `index_data = {}` (dictionary mapping `rel_path -> file_hash`).
          * For each file under `temp_dir/node_modules/` (or `temp_dir/vendor/`):

            * `rel_path = relative path from temp_dir/node_modules` (or `vendor`).
            * `file_hash = blob_storage.save_blob(temp_dir/<output_folder>/<rel_path>)`.
            * `index_data[rel_path] = file_hash`.
       5. **Save Index**:

          * Index path:

            ```
            cache/indexes/<bundle_hash>.<manager>.<manager_version>.index
            ```
          * Save `index_data` as JSON to that path.
       6. **Generate ZIP**:

          * `zip_path = cache_dir/"bundles"/f"{bundle_hash}.zip"`.
          * For each `rel_path, file_hash` in `index_data`:

            * `blob_bytes = blob_storage.read_blob(file_hash)`.
            * Add it to the ZIP with `arcname = rel_path`.
       7. **Respond**:

          ```jsonc
          { "download_url": "http://.../download/<bundle_hash>.zip", "cache_hit": false }
          ```
       8. **Cleanup**: `shutil.rmtree(temp_dir, ignore_errors=True)`.

3. **Client**:

   * Receives `download_url` and `cache_hit`.
   * Whether `cache_hit=true` or `false`, it downloads the ZIP.
   * Extracts it into `node_modules/` or `vendor/`.

4. **ZIP Download**:

   * Client does `GET /download/<bundle_hash>.zip`.
   * Server, in `download_endpoint(bundle_hash)`, checks if `cache/bundles/<bundle_hash>.zip` exists:

     * If it exists, return a `FileResponse` with that ZIP.
     * Otherwise, return `404 Not Found`.

---

## 8. Implementation Details and Pseudocode

### 8.1 Client Pseudocode

```python
#!/usr/bin/env python3
# client/cli.py

import argparse
import base64
import hashlib
import json
import os
import shutil
import sys
import tempfile
import zipfile
import requests

from .hash_calculator import HASH_ALGORITHM, HASH_BLOCK_SIZE

class HashCalculator:
    @staticmethod
    def calculate_hash(manager: str, file_paths: list[str], versions: dict) -> str:
        """
        Calculates SHA256 with:
          1. manager
          2. content of files (sorted alphabetically)
          3. versions (sorted by key)
        """
        sha = hashlib.new(HASH_ALGORITHM)
        sha.update(manager.encode("utf-8"))
        sha.update(b"\n")
        for file_name in sorted(file_paths):
            with open(file_name, "rb") as f:
                while True:
                    chunk = f.read(HASH_BLOCK_SIZE)
                    if not chunk:
                        break
                    sha.update(chunk)
        for k in sorted(versions.keys()):
            v = versions[k]
            sha.update(f"{k}={v}\n".encode("utf-8"))
        return sha.hexdigest()

def parse_args():
    parser = argparse.ArgumentParser(description="DepCacheProxy CLI Client")
    parser.add_argument("endpoint_url", type=str, help="Base URL of the server (http://host:port/api)")
    parser.add_argument("manager", type=str, help="Dependency manager (npm, composer, etc.)")
    parser.add_argument("--apikey", type=str, required=False, help="API Key (if required)")
    parser.add_argument("--files", type=str, required=True, help="Files: package.json,package.lock or composer.json,composer.lock")
    parser.add_argument("--node-version", type=str, required=False, help="NodeJS version (npm only)")
    parser.add_argument("--npm-version", type=str, required=False, help="NPM version (npm only)")
    parser.add_argument("--php-version", type=str, required=False, help="PHP version (composer only)")
    parser.add_argument("--timeout", type=int, default=60, help="HTTP timeout in seconds")
    return parser.parse_args()

def main():
    args = parse_args()
    endpoint = args.endpoint_url.rstrip("/")
    manager = args.manager.lower()
    api_key = args.apikey

    supported = ["npm", "composer"]
    if manager not in supported:
        print(f"ERROR: Manager '{manager}' not supported. Options: {', '.join(supported)}")
        sys.exit(1)

    file_list = [f.strip() for f in args.files.split(",")]
    if len(file_list) < 2:
        print("ERROR: You must specify at least two files: definition + lockfile")
        sys.exit(1)
    for fp in file_list:
        if not os.path.isfile(fp):
            print(f"ERROR: File not found: {fp}")
            sys.exit(1)

    versions = {}
    if manager == "npm":
        if not args.node_version or not args.npm_version:
            print("ERROR: You must specify --node-version and --npm-version for npm")
            sys.exit(1)
        versions["node"] = args.node_version
        versions["npm"] = args.npm_version
    elif manager == "composer":
        if not args.php_version:
            print("ERROR: You must specify --php-version for composer")
            sys.exit(1)
        versions["php"] = args.php_version

    # Calculate bundle hash
    bundle_hash = HashCalculator.calculate_hash(manager, file_list, versions)
    print(f"[INFO] Bundle hash: {bundle_hash}")

    # Read and base64-encode files
    files_b64 = {}
    for fp in file_list:
        with open(fp, "rb") as f:
            files_b64[os.path.basename(fp)] = base64.b64encode(f.read()).decode("utf-8")

    payload = {
        "manager": manager,
        "hash": bundle_hash,
        "files": files_b64,
        "versions": versions
    }
    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"

    url_cache = f"{endpoint}/v1/cache"
    print(f"[INFO] Sending request to {url_cache} ...")
    try:
        resp = requests.post(url_cache, headers=headers, data=json.dumps(payload), timeout=args.timeout)
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] HTTP connection failed: {e}")
        sys.exit(1)

    if resp.status_code == 401:
        print("ERROR: Unauthorized. Check your API Key.")
        sys.exit(1)
    elif resp.status_code != 200:
        print(f"ERROR: Server returned status {resp.status_code}: {resp.text}")
        sys.exit(1)

    resp_data = resp.json()
    download_url = resp_data.get("download_url")
    cache_hit = resp_data.get("cache_hit", False)
    if not download_url:
        print("ERROR: Invalid server response (missing download_url).")
        sys.exit(1)

    if cache_hit:
        print("[INFO] Cache hit: downloading ZIP...")
    else:
        print("[INFO] Cache miss: generating dependencies. Downloading when ready...")

    # Download ZIP
    try:
        zip_resp = requests.get(download_url, stream=True, timeout=args.timeout)
        if zip_resp.status_code != 200:
            print(f"ERROR: Failed to download ZIP (status {zip_resp.status_code})")
            sys.exit(1)
        tmp_zip = tempfile.NamedTemporaryFile(delete=False, suffix=".zip")
        with open(tmp_zip.name, "wb") as f:
            for chunk in zip_resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        print(f"[INFO] ZIP downloaded to {tmp_zip.name}")
    except requests.exceptions.RequestException as e:
        print(f"[ERROR] Failed to download ZIP: {e}")
        sys.exit(1)

    # Extract ZIP into local dependency folder
    if manager == "npm":
        target_dir = os.path.join(os.getcwd(), "node_modules")
    elif manager == "composer":
        target_dir = os.path.join(os.getcwd(), "vendor")
    else:
        target_dir = os.path.join(os.getcwd(), "deps")

    if os.path.isdir(target_dir):
        print(f"[INFO] Removing existing folder: {target_dir}")
        shutil.rmtree(target_dir)

    os.makedirs(target_dir, exist_ok=True)
    with zipfile.ZipFile(tmp_zip.name, "r") as zip_ref:
        zip_ref.extractall(target_dir)
    print(f"[INFO] Dependencies extracted to: {target_dir}")

    os.remove(tmp_zip.name)
    print("[INFO] Process completed successfully.")
    sys.exit(0)

if __name__ == "__main__":
    main()
```

---

### 8.2 Server Pseudocode

```python
#!/usr/bin/env python3
# server/interfaces/main.py

import argparse
import base64
import hashlib
import json
import os
import shutil
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Optional

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel

from server.domain.hash_constants import HASH_ALGORITHM, HASH_BLOCK_SIZE
from server.domain.dependency_set import DependencySet
from server.domain.blob_storage import BlobStorage
from server.domain.cache_repository import ICacheRepository, CacheObject
from server.infrastructure.file_system_cache_repository import FileSystemCacheRepository
from server.infrastructure.api_key_validator import ApiKeyValidator
from server.infrastructure.docker_utils import run_in_docker
from server.domain.installer import InstallerFactory
from server.domain.zip_util import ZipUtil

# ----------------------------
# Application: DTOs
# ----------------------------
class CacheRequestDTO(BaseModel):
    manager: str
    hash: str
    files: Dict[str, str]       # {"package.json": "<base64>", ...}
    versions: Dict[str, str]    # {"node":"14.20.0","npm":"6.14.13"} or {"php":"8.1.0"}

class CacheResponseDTO(BaseModel):
    download_url: str
    cache_hit: bool

# ----------------------------
# Application: HandleCacheRequest
# ----------------------------
class HandleCacheRequest:
    def __init__(
        self,
        cache_repo: ICacheRepository,
        blob_storage: BlobStorage,
        installer_factory: InstallerFactory,
        zip_util: ZipUtil,
        supported_versions_node: Dict[str, str],
        supported_versions_php: List[str],
        use_docker: bool
    ):
        self.cache_repo = cache_repo
        self.blob_storage = blob_storage
        self.installer_factory = installer_factory
        self.zip_util = zip_util
        self.supported_versions_node = supported_versions_node
        self.supported_versions_php = supported_versions_php
        self.use_docker = use_docker

    def execute(self, request_dto: CacheRequestDTO, base_download_url: str) -> CacheResponseDTO:
        manager = request_dto.manager
        bundle_hash = request_dto.hash
        versions = request_dto.versions

        # Determine manager_version
        if manager == "npm":
            node_ver = versions.get("node")
            npm_ver = versions.get("npm")
            if not node_ver or not npm_ver:
                raise RuntimeError("Missing Node/NPM versions")
            manager_version = f"{node_ver}_{npm_ver}"
            # If no supported versions are specified, accept any version
            if self.supported_versions_node:
                supported_npm_ver = self.supported_versions_node.get(node_ver)
                if supported_npm_ver != npm_ver:
                    if not self.use_docker:
                        raise ValueError("Node/NPM version not supported")
                    else:
                        use_docker_exec = True
                else:
                    use_docker_exec = False
            else:
                use_docker_exec = False
        elif manager == "composer":
            php_ver = versions.get("php")
            if not php_ver:
                raise RuntimeError("Missing PHP version")
            manager_version = php_ver
            # If no supported versions are specified, accept any version
            if self.supported_versions_php:
                if php_ver not in self.supported_versions_php:
                    if not self.use_docker:
                        raise ValueError("PHP version not supported")
                    else:
                        use_docker_exec = True
                else:
                    use_docker_exec = False
            else:
                use_docker_exec = False
        else:
            raise ValueError(f"Manager not implemented: {manager}")

        # Check if ZIP exists
        if self.cache_repo.exists_bundle(bundle_hash):
            download_url = f"{base_download_url}/download/{bundle_hash}.zip"
            return CacheResponseDTO(download_url=download_url, cache_hit=True)

        # Cache Miss: generate
        temp_dir = Path(tempfile.mkdtemp(prefix=bundle_hash))
        try:
            # Write decoded files into temp_dir
            for fname, b64 in request_dto.files.items():
                data = base64.b64decode(b64)
                (temp_dir / fname).write_bytes(data)

            # Install dependencies (local or Docker)
            installer = self.installer_factory.get_installer(manager, versions)
            if use_docker_exec:
                run_in_docker(temp_dir, manager, versions)
            else:
                installer.install(temp_dir)

            # 1) Blobify files
            index_data: Dict[str, str] = {}
            output_folder = installer.output_folder_name  # "node_modules" or "vendor"
            for dirpath, _, filenames in os.walk(temp_dir / output_folder):
                for fn in filenames:
                    full_path = Path(dirpath) / fn
                    rel_path = str(full_path.relative_to(temp_dir / output_folder))
                    file_hash = self.blob_storage.save_blob(full_path)
                    index_data[rel_path] = file_hash

            # 2) Save index
            self.cache_repo.save_index(bundle_hash, manager, manager_version, index_data)

            # 3) Generate ZIP from blobs
            zip_path = self.cache_repo.get_bundle_zip_path(bundle_hash)
            self.zip_util.create_zip_from_blobs(zip_path, index_data, self.blob_storage)

            return CacheResponseDTO(
                download_url=f"{base_download_url}/download/{bundle_hash}.zip",
                cache_hit=False
            )

        except Exception as e:
            # Cleanup in case of error
            shutil.rmtree(temp_dir, ignore_errors=True)
            raise RuntimeError(f"Error processing bundle: {e}")

        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)

# ----------------------------
# Interfaces / Entry Point: FastAPI
# ----------------------------
def create_app(
    cache_dir: Path,
    is_public: bool,
    api_keys: List[str],
    supported_versions_node: Dict[str, str],
    supported_versions_php: List[str],
    use_docker_on_mismatch: bool,
    base_download_url: str
) -> FastAPI:
    app = FastAPI()

    # Initialize repositories and infrastructure
    objects_dir = cache_dir / "objects"
    indexes_dir = cache_dir / "indexes"
    bundles_dir = cache_dir / "bundles"
    objects_dir.mkdir(parents=True, exist_ok=True)
    indexes_dir.mkdir(parents=True, exist_ok=True)
    bundles_dir.mkdir(parents=True, exist_ok=True)

    cache_repo = FileSystemCacheRepository(cache_dir)
    blob_storage = BlobStorage(objects_dir)
    installer_factory = InstallerFactory()
    zip_util = ZipUtil()
    validator = ApiKeyValidator(api_keys) if not is_public else None

    handler = HandleCacheRequest(
        cache_repo, blob_storage, installer_factory, zip_util,
        supported_versions_node, supported_versions_php, use_docker_on_mismatch
    )

    @app.post("/v1/cache", response_model=CacheResponseDTO)
    async def cache_endpoint(request: Request):
        # Validate API Key if not public
        if not is_public:
            auth: Optional[str] = request.headers.get("Authorization")
            if not auth or not auth.startswith("Bearer "):
                raise HTTPException(status_code=401, detail="Missing Authorization Bearer <APIKEY>")
            key = auth.split(" ")[1]
            if not validator.validate(key):
                raise HTTPException(status_code=401, detail="Invalid API Key")

        # Parse payload
        payload = await request.json()
        try:
            dto = CacheRequestDTO(**payload)
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Invalid payload: {e}")

        supported_managers = ["npm", "composer"]
        if dto.manager not in supported_managers:
            raise HTTPException(status_code=400, detail=f"Unsupported manager: {dto.manager}")

        try:
            response_dto = handler.execute(dto, base_download_url)
        except ValueError as ve:
            raise HTTPException(status_code=400, detail=str(ve))
        except RuntimeError as re:
            raise HTTPException(status_code=500, detail=str(re))

        return JSONResponse(status_code=200, content=response_dto.dict())

    @app.get("/download/{bundle_hash}.zip")
    async def download_endpoint(bundle_hash: str):
        zip_path = cache_dir / "bundles" / f"{bundle_hash}.zip"
        if zip_path.is_file():
            return FileResponse(zip_path, filename=f"{bundle_hash}.zip", media_type="application/zip")
        raise HTTPException(status_code=404, detail="ZIP not found")

    return app

def parse_args():
    parser = argparse.ArgumentParser(description="DepCacheProxy Server")
    parser.add_argument("port", type=int, help="HTTP port to listen on (e.g., 8080)")
    parser.add_argument("--cache_dir", type=str, required=True, help="Base cache directory")
    parser.add_argument(
        "--supported-versions-node", type=str, required=False,
        help="Pairs node_version:npm_version separated by commas, e.g. 14.20.0:6.14.13,16.15.0:8.5.0 (optional)"
    )
    parser.add_argument(
        "--supported-versions-php", type=str, required=False,
        help="List of PHP versions separated by commas, e.g. 8.1.0,7.4.0 (optional)"
    )
    parser.add_argument(
        "--use-docker-on-version-mismatch",
        action="store_true",
        help="Use Docker to install dependencies if version is unsupported"
    )
    parser.add_argument("--is_public", action="store_true", default=False, help="Public server (no API key required)")
    parser.add_argument("--api-keys", type=str, required=False, help="Comma-separated list of valid API keys")
    return parser.parse_args()

def main():
    args = parse_args()
    port = args.port
    cache_dir = Path(args.cache_dir)
    use_docker = args.use_docker_on_version_mismatch
    is_public = args.is_public

    supported_versions_node = {}
    if args.supported_versions_node:
        for pair in args.supported_versions_node.split(","):
            node_v, npm_v = pair.split(":")
            supported_versions_node[node_v.strip()] = npm_v.strip()

    supported_versions_php = []
    if args.supported_versions_php:
        supported_versions_php = [v.strip() for v in args.supported_versions_php.split(",")]

    api_keys = []
    if not is_public:
        if not args.api_keys:
            print("ERROR: --api-keys must be provided when not public.")
            sys.exit(1)
        api_keys = [k.strip() for k in args.api_keys.split(",") if k.strip()]

    # Create cache directory structure
    (cache_dir / "objects").mkdir(parents=True, exist_ok=True)
    (cache_dir / "indexes").mkdir(parents=True, exist_ok=True)
    (cache_dir / "bundles").mkdir(parents=True, exist_ok=True)

    base_download_url = f"http://localhost:{port}"
    app = create_app(
        cache_dir, is_public, api_keys,
        supported_versions_node, supported_versions_php, use_docker, base_download_url
    )
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=port)

if __name__ == "__main__":
    main()
```

---

### 8.3 Common Helper Functions

#### 8.3.1 `ZipUtil` for ZIP from Blobs (`zip_util.py`)

```python
# server/domain/zip_util.py
import zipfile
from pathlib import Path
from typing import Dict
from .blob_storage import BlobStorage

class ZipUtil:
    @staticmethod
    def create_zip_from_blobs(zip_path: Path, index_data: Dict[str, str], blob_storage: BlobStorage) -> None:
        """
        Creates a ZIP at zip_path. For each (relative_path, file_hash)
        in index_data, read the blob via blob_storage.read_blob(file_hash)
        and add it to the ZIP with arcname=relative_path.
        """
        zip_path.parent.mkdir(parents=True, exist_ok=True)
        with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
            for rel_path, file_hash in index_data.items():
                blob_bytes = blob_storage.read_blob(file_hash)
                # To add bytes, use writestr
                zf.writestr(rel_path, blob_bytes)
```

---

## 9. HTTP API and Route Schema

### 9.1 Routes

| Method | Route                         | Description                                                | Request Type        | Responses                                                                                                     |
| ------ | ----------------------------- | ---------------------------------------------------------- | ------------------- | ------------------------------------------------------------------------------------------------------------- |
| POST   | `/v1/cache`                   | Request cache of dependencies: hit/miss and download URL.  | Multipart Form Data | `200 OK` → `CacheResponseDTO` <br> `400 Bad Request` <br> `401 Unauthorized` <br> `500 Internal Server Error` |
| GET    | `/download/{bundle_hash}.zip` | Download the generated ZIP for the specified bundle\_hash. | None                | `200 OK` → ZIP <br> `404 Not Found`                                                                           |

### 9.2 `CacheRequestDTO` (Multipart Form Data)

The `/v1/cache` endpoint accepts multipart form data with the following fields:

* `manager` (string): Package manager name (e.g., "npm", "composer") - required
* `hash` (string): Pre-calculated bundle hash - required
* `versions` (string): JSON string containing version information - required
  * For `npm`: `{"node": "14.20.0", "npm": "6.14.13"}`
  * For `composer`: `{"php": "8.1.0"}`
* `custom_args` (string): JSON array of custom installation arguments - optional
  * Example: `["--legacy-peer-deps", "--force"]` for npm
  * Example: `["--no-dev", "--optimize-autoloader"]` for composer
* `file` (file): Multiple file uploads - required
  * Must include the manifest file (package.json for npm, composer.json for composer)
  * Lockfile is optional:
    * For `npm`: package-lock.json (if missing, npm install will be run)
    * For `composer`: composer.lock (always optional)

Example curl requests:

1. npm with lockfile:
```bash
curl -X POST http://localhost:8080/v1/cache \
  -H "Authorization: Bearer API_KEY" \
  -F "manager=npm" \
  -F "hash=ab12cd34ef56..." \
  -F 'versions={"node":"14.20.0","npm":"6.14.13"}' \
  -F "file=@package.json" \
  -F "file=@package-lock.json"
```

2. npm without lockfile (will run npm install):
```bash
curl -X POST http://localhost:8080/v1/cache \
  -H "Authorization: Bearer API_KEY" \
  -F "manager=npm" \
  -F "hash=ab12cd34ef56..." \
  -F 'versions={"node":"14.20.0","npm":"6.14.13"}' \
  -F "file=@package.json"
```

3. npm with custom arguments:
```bash
curl -X POST http://localhost:8080/v1/cache \
  -H "Authorization: Bearer API_KEY" \
  -F "manager=npm" \
  -F "hash=ab12cd34ef56..." \
  -F 'versions={"node":"14.20.0","npm":"6.14.13"}' \
  -F 'custom_args=["--legacy-peer-deps","--force"]' \
  -F "file=@package.json" \
  -F "file=@package-lock.json"
```

4. composer (lockfile always optional):
```bash
curl -X POST http://localhost:8080/v1/cache \
  -H "Authorization: Bearer API_KEY" \
  -F "manager=composer" \
  -F "hash=ab12cd34ef56..." \
  -F 'versions={"php":"8.1.0"}' \
  -F "file=@composer.json"
```

5. composer with custom arguments:
```bash
curl -X POST http://localhost:8080/v1/cache \
  -H "Authorization: Bearer API_KEY" \
  -F "manager=composer" \
  -F "hash=ab12cd34ef56..." \
  -F 'versions={"php":"8.1.0"}' \
  -F 'custom_args=["--no-dev","--optimize-autoloader"]' \
  -F "file=@composer.json"
```

### 9.3 `CacheResponseDTO`

```jsonc
{
  "download_url": "http://server:8080/download/ab12cd34ef56....zip",
  "cache_hit": true
}
```

* `download_url`: string (required).
* `cache_hit`: boolean (required).

---

## 10. Test Structure

```
tests/
├── unit/
│   ├── test_hash_calculator.py
│   ├── test_api_key_validator.py
│   ├── test_installer_factory.py
│   ├── test_blob_storage.py
│   └── test_zip_util.py
├── integration/
│   ├── test_handle_cache_request_hit.py
│   ├── test_handle_cache_request_miss.py
│   └── test_docker_installation.py
├── functional/
│   ├── test_cli_client_request.py
│   └── test_server_download_endpoint.py
└── e2e/
    ├── test_end_to_end_npm.py
    └── test_end_to_end_composer.py
```

### 10.1 Unit Tests

* **`test_hash_calculator.py`**:

  * Verify that identical inputs produce the same hash.
  * Verify that changing one byte in `package.json` changes the hash.

* **`test_api_key_validator.py`**:

  * `validate(valid_key) == True`.
  * `validate(invalid_key) == False`.

* **`test_installer_factory.py`**:

  * `get_installer("npm", ...)` → returns `NpmInstaller` instance.
  * `get_installer("composer", ...)` → returns `ComposerInstaller` instance.
  * `get_installer("other", ...)` raises `ValueError`.

* **`test_blob_storage.py`**:

  * Create a temporary file with fixed content. `save_blob` returns a `file_hash`.
  * Verify that `get_blob_path(file_hash)` exists and its content matches.
  * Calling `save_blob` again does not overwrite (content remains the same).

* **`test_zip_util.py`**:

  * Given a test index pointing to blobs generated by `blob_storage`, call `create_zip_from_blobs` and verify the ZIP contains files with correct paths and valid content.

### 10.2 Integration Tests

* **`test_handle_cache_request_hit.py`**:

  * Precondition: generate a bundle in advance (run `handler.execute` on a request).
  * Call `handler.execute` again with the same `bundle_hash` and verify it returns `cache_hit=True` without creating new blobs or index files.

* **`test_handle_cache_request_miss.py`**:

  * Simulate a `CacheRequestDTO` for a new bundle.
  * After `execute`, verify:

    * Blobs exist in `cache/objects/`.
    * JSON index exists in `cache/indexes/`.
    * ZIP exists in `cache/bundles/`.
  * Verify `cache_hit=False`.

* **`test_docker_installation.py`**:

  * Configure `use_docker_on_mismatch=True` and pass unsupported versions.
  * Verify that `run_in_docker` creates `node_modules/` or `vendor/` in the temporary directory and then blobs them.

### 10.3 Functional Tests

* **`test_cli_client_request.py`**:

  * Use FastAPI’s `TestClient` to spin up the server in memory.
  * Run the CLI (`subprocess.run([...])`) pointing to that server.
  * Verify the CLI prints expected messages and that after completing, a `node_modules/` folder exists locally with content.

* **`test_server_download_endpoint.py`**:

  * Manually insert a ZIP into `cache/bundles/<bundle_hash>.zip`.
  * Perform `GET /download/<bundle_hash>.zip` and validate that the response content matches the inserted ZIP byte-for-byte.

### 10.4 End-to-End Tests

* **`test_end_to_end_npm.py`**:

  * Spin up the server Docker container.
  * Mount a local volume as `/cache`.
  * Run the CLI in another container or host, pointing to the server Docker.
  * Verify that the final `node_modules/` matches a known test fixture.

* **`test_end_to_end_composer.py`**:

  * Analogous for Composer.

---

## 11. Use Cases and Test Scenarios

### 11.1 Scenario 1: NPM Cache Hit

1. **Server** starts:

   ```bash
   dep_cache_proxy_server 8080 \
     --cache_dir=./cache \
     --supported-versions-node=14.20.0:6.14.13,16.15.0:8.5.0 \
     --supported-versions-php=8.1.0 \
     --use-docker-on-version-mismatch \
     --is_public
   ```
2. **Existing Bundle**:
   Suppose `bundle_hash = "ab12cd34..."` was already processed, and the following exist:

   * Blobs in `cache/objects/...`.
   * Index `cache/indexes/ab12cd34.npm.14.20.0_6.14.13.index`.
   * ZIP in `cache/bundles/ab12cd34.zip`.
3. **Client**:

   ```bash
   dep_cache_proxy_client http://localhost:8080/api npm \
     --files=package.json,package.lock \
     --node-version=14.20.0 \
     --npm-version=6.14.13
   ```

   * Calculates `bundle_hash = "ab12cd34..."`.
   * Sends `POST /v1/cache`.
   * Server detects the existing ZIP → responds:

     ```jsonc
     { "download_url": "http://localhost:8080/download/ab12cd34.zip", "cache_hit": true }
     ```
   * Client downloads and extracts into `./node_modules/`.

---

### 11.2 Scenario 2: Composer Cache Miss

1. **Server**:

   ```bash
   dep_cache_proxy_server 8080 \
     --cache_dir=./cache \
     --supported-versions-node=14.20.0:6.14.13 \
     --supported-versions-php=8.1.0,7.4.0 \
     --api-keys=KEY1
   ```
2. **Client**:

   ```bash
   dep_cache_proxy_client http://localhost:8080/api composer \
     --apikey=KEY1 \
     --files=composer.json,composer.lock \
     --php-version=8.1.0
   ```

   * Calculates `bundle_hash = "aa11bb22..."`.
   * Server sees ZIP does not exist.
   * Creates `temp_dir`, writes files, runs `composer install --no-dev --prefer-dist --no-scripts`.
   * In `temp_dir/vendor/`, blobs each file:

     * E.g., `vendor/packageA/index.php` → `file_hash = "eecc1232..."`.
     * Saved to `cache/objects/ee/cc/eecc1232...`.
     * `index_data["packageA/index.php"] = "eecc1232..."`.
   * Saves JSON index in:

     ```
     cache/indexes/aa11bb22.composer.8.1.0.index
     ```
   * Generates ZIP in `cache/bundles/aa11bb22.zip` using blobs:

     * Inside the ZIP, `packageA/index.php` has content from its blob.
   * Responds:

     ```jsonc
     { "download_url": "http://localhost:8080/download/aa11bb22.zip", "cache_hit": false }
     ```
3. **Client**:

   * Downloads and extracts into `./vendor/`.

---

### 11.3 Scenario 3: Unsupported Version without Docker

1. **Server**:

   ```bash
   dep_cache_proxy_server 8080 \
     --cache_dir=./cache \
     --supported-versions-node=14.20.0:6.14.13 \
     --supported-versions-php=8.1.0 \
     --api-keys=KEY1
   ```

   * `use_docker_on_version_mismatch=false`.
2. **Client**:

   ```bash
   dep_cache_proxy_client http://localhost:8080/api npm \
     --apikey=KEY1 \
     --files=package.json,package.lock \
     --node-version=16.15.0 \
     --npm-version=8.5.0
   ```

   * Calculates `bundle_hash`.
   * Server sees mismatch `(16.15.0,8.5.0)` and `use_docker=false` → returns `400 Bad Request` with detail `"Node/NPM version not supported"`.
3. **Client**:

   * Receives `resp.status_code == 400` and exits with error.

---

### 11.4 Scenario 4: Unsupported Version with Docker

1. **Server**:

   ```bash
   dep_cache_proxy_server 8080 \
     --cache_dir=./cache \
     --supported-versions-node=14.20.0:6.14.13 \
     --supported-versions-php=8.1.0 \
     --use-docker-on-version-mismatch \
     --api-keys=KEY1
   ```
2. **Client**:

   ```bash
   dep_cache_proxy_client http://localhost:8080/api npm \
     --apikey=KEY1 \
     --files=package.json,package.lock \
     --node-version=16.15.0 \
     --npm-version=8.5.0
   ```

   * Server detects mismatch and `use_docker=true`.
   * In `temp_dir`, runs:

     ```bash
     docker run --rm \
       -v <temp_dir>:/usr/src/app \
       -w /usr/src/app \
       node:16.15.0 \
       sh -c "npm ci --ignore-scripts --no-audit --cache .npm_cache"
     ```
   * Continues to blobify, index, and generate ZIP.
   * Responds with `cache_hit=false`.

---

## 12. Security, Scalability, and Common Pitfalls

### 12.1 Security

1. **Input Validation**:

   * `manager` must be whitelisted.
   * Validate `versions` with SemVer regex.

2. **Command Execution**:

   * Local: use `subprocess.run([...])` with an explicit list of arguments.
   * Docker: use a list in `run_in_docker`, avoiding unsafe interpolations.

3. **API Key**:

   * Compare securely (timing-safe).
   * Do not log `--api-keys` values.

4. **ZIP and Blobs**:

   * Only serve files from `cache/bundles` and `cache/objects`.
   * Do not expose filesystem paths outside `<cache_dir>`.

### 12.2 Scalability

1. **Concurrency**:

   * Lock by `bundle_hash` during a “miss” flow:

     * Create a lock file (`cache_dir/locks/<bundle_hash>.lock`) and use `fcntl.flock`.
     * Other processes wait until the lock is released.

2. **Cache Retention**:

   * Cronjob to delete bundles older than X days without access.
   * Optionally maintain an access counter in metadata.

3. **Distributed Storage**:

   * Replace `FileSystemCacheRepository` with `S3CacheRepository`.
   * Adjust `blob_storage` to use S3.

### 12.3 Common Pitfalls

1. **Hash Mismatch**:

   * Ensure client and server share the same constants.
   * Confirm alphabetical ordering of files and versions.

2. **Docker Failures**:

   * Ensure Docker is installed and accessible.
   * Confirm the image (e.g., `node:16.15.0`) is either present locally or can be pulled.

3. **Directory Permissions**:

   * The process must be able to write to `<cache_dir>`.

4. **Temporary Directory Not Removed**:

   * In all flows, call `shutil.rmtree(temp_dir, ignore_errors=True)` in a finally block.

---

## 13. Ease of Adding New Managers

To add a new manager (e.g., Yarn, Pip), follow these steps:

1. **Create a subclass of `DependencyInstaller` in `server/domain/installer.py`:**

   ```python
   class YarnInstaller(DependencyInstaller):
       @property
       def output_folder_name(self) -> str:
           return "node_modules"  # if Yarn places dependencies there

       def install(self, work_dir: Path) -> None:
           cmd = ["yarn", "install", "--frozen-lockfile"]
           process = subprocess.run(cmd, cwd=str(work_dir), capture_output=True)
           if process.returncode != 0:
               raise RuntimeError(f"Yarn install failed: {process.stderr.decode()}")

       def install_with_docker(self, work_dir: Path, versions: Dict[str, str]) -> None:
           node_ver = versions.get("node")
           if not node_ver:
               raise RuntimeError("Must specify node_version for Yarn in Docker")
           cmd = [
               "docker", "run", "--rm",
               "-v", f"{work_dir}:/usr/src/app",
               "-w", "/usr/src/app",
               f"node:{node_ver}",
               "sh", "-c", "yarn install --frozen-lockfile"
           ]
           process = subprocess.run(cmd, capture_output=True)
           if process.returncode != 0:
               raise RuntimeError(f"Yarn install in Docker failed: {process.stderr.decode()}")
   ```

2. **Update `InstallerFactory`:**

   ```python
   class InstallerFactory:
       def get_installer(self, manager: str, versions: Dict[str, str]) -> DependencyInstaller:
           if manager == "npm":
               return NpmInstaller(versions)
           elif manager == "composer":
               return ComposerInstaller(versions)
           elif manager == "yarn":
               return YarnInstaller(versions)
           else:
               raise ValueError(f"Installer not implemented for manager '{manager}'")
   ```

3. **Modify Manager Validation**:

   * In `cache_endpoint`, include `"yarn"` in `supported_managers`.
   * For version validation, require `node_version` for Yarn (similar to NPM).

4. **Update Version Configuration**:

   * `--supported-versions-node` can be used for both NPM and Yarn.

5. **Add Tests**:

   * `tests/unit/test_installer_factory.py`: verify that `"yarn"` returns a `YarnInstaller` instance.
   * `tests/integration/test_handle_cache_request_yarn.py`: “hit/miss” flow for Yarn.
   * Functional and E2E tests analogous to NPM.

---

## 14. Conclusions

This updated analysis corrects the storage strategy in `cache/objects` so that it contains **only individual file blobs** indexed by relative path → file hash. Each bundle’s index resides in `cache/indexes`, and the final ZIPs in `cache/bundles`. Version requirements and optional Docker usage remain as designed, along with the hash constants, DDD + SOLID separation, and a thorough testing strategy (unit, integration, functional, and E2E). This documentation should facilitate implementation, maintenance, and adding new dependency managers.